{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This jupyter notebook is for assistant maintenance purposes only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the necessary libraries\n",
    "from openai import OpenAI\n",
    "from typing_extensions import override\n",
    "from openai import AssistantEventHandler\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventHandler(AssistantEventHandler):\n",
    "    @override\n",
    "    def on_text_created(self, text) -> None:\n",
    "        print(f\"\\nassistant > \", end=\"\", flush=True)\n",
    "\n",
    "    @override\n",
    "    def on_tool_call_created(self, tool_call):\n",
    "        print(f\"\\nassistant > {tool_call.type}\\n\", flush=True)\n",
    "\n",
    "    @override\n",
    "    def on_message_done(self, message) -> None:\n",
    "        # print a citation to the file searched\n",
    "        message_content = message.content[0].text\n",
    "        annotations = message_content.annotations\n",
    "        citations = []\n",
    "        for index, annotation in enumerate(annotations):\n",
    "            message_content.value = message_content.value.replace(\n",
    "                annotation.text, f\"[{index}]\"\n",
    "            )\n",
    "            if file_citation := getattr(annotation, \"file_citation\", None):\n",
    "                cited_file = client.files.retrieve(file_citation.file_id)\n",
    "                citations.append(f\"[{index}] {cited_file.filename}\")\n",
    "\n",
    "        print(message_content.value)\n",
    "        print(\"\\n\".join(citations))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the assistant with file_search enabled\n",
    "\n",
    "Our first step is to create an Assistant that can do file searching regardless of where the vector store resides (Assistant or Thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the Assistant\n",
    "\n",
    "assistant = client.beta.assistants.create(\n",
    "    model=\"gpt-4o\",\n",
    "    \n",
    "    instructions=\"\"\"\n",
    "        You are BisonGPT, a knowledgeable and supportive assistant dedicated to helping students at Howard University. You have access to a wide range of information about Howard, stored in various documents and files, which you can search to provide specific, accurate answers.\n",
    "\n",
    "        Your main role is to assist students by answering questions, offering academic advice, and helping them navigate university processes. You can:\n",
    "        - Recommend course schedules tailored to students' academic plans and graduation requirements.\n",
    "        - Advise on graduation plans and pathways, based on program-specific requirements and deadlines.\n",
    "        - Explain university policies, including financial aid, academic advising, and admissions.\n",
    "        - Guide students on campus resources, student organizations, and events.\n",
    "\n",
    "        When responding, aim to be clear, concise, and supportive. Reference the university's guidelines and the most relevant information available in your files to ensure your answers are both accurate and useful. Adapt your tone to be professional but friendly, as you are here to encourage and empower students on their academic journey.\n",
    "    \"\"\",\n",
    "    \n",
    "    name=\"BisonGPT\",\n",
    "    \n",
    "    tools=[{\"type\": \"file_search\"}],\n",
    "    \n",
    "    metadata={\n",
    "        \"can_be_used_for_file_search\": \"True\",\n",
    "        \"can_hold_vector_store\": \"True\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a vector store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n",
      "FileCounts(cancelled=0, completed=3, failed=0, in_progress=0, total=3)\n"
     ]
    }
   ],
   "source": [
    "vector_store = client.beta.vector_stores.create(name=\"Howard University Data\")\n",
    "\n",
    "def get_all_files(directory_path):\n",
    "    file_paths = []\n",
    "    for root, _, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            file_paths.append(os.path.join(root, file))\n",
    "    return file_paths\n",
    "\n",
    "files = get_all_files(\"../Data/\")\n",
    "\n",
    "file_streams = [open(path, \"rb\") for path in files]\n",
    "\n",
    "file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "  vector_store_id=vector_store.id, files=file_streams\n",
    ")\n",
    " \n",
    "# You can print the status and the file counts of the batch to see the result of this operation.\n",
    "print(file_batch.status)\n",
    "print(file_batch.file_counts)\n",
    "\n",
    "#close the files\n",
    "\n",
    "for file in file_streams:\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Or update the vector store with new files\n",
    "\n",
    "Still testing DO NOT RUN THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Retrieve and clear out current files\n",
    "\n",
    "# vector_store_files = client.beta.vector_stores.files.list(vector_store_id=vector_store.id)\n",
    "\n",
    "# for file in vector_store_files.data:\n",
    "#     client.beta.vector_stores.files.delete(\n",
    "#         vector_store_id=vector_store.id, file_id=file.id\n",
    "#     )\n",
    "\n",
    "# #Get all the new files\n",
    "\n",
    "# files = get_all_files(\"../Data/\")\n",
    "\n",
    "# file_streams = [open(path, \"rb\") for path in files]\n",
    "\n",
    "# file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "#     vector_store_id=vector_store.id, files=file_streams\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attach the vector store to the assisstant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    assistant = client.beta.assistants.update(\n",
    "        assistant_id=assistant.id,\n",
    "        tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Failed to attach vector store to assistant: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
